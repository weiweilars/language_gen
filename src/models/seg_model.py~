import os
import yaml
import cv2
import numpy as np
from typing import Any, Dict, Optional, Union
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import pytorch_lightning
from help_function import object_from_dict, state_dict_from_disk, mean_iou, find_average

import segmentation_models_pytorch
from segmentation_models_pytorch.losses import JaccardLoss, DiceLoss, SoftCrossEntropyLoss, FocalLoss
from src.segmentation_models_pytorch.utils.metrics import IoU
from dataset import SegDataset

class SegTrain(pytorch_lightning.LightningModule):

    # def __init__(self, **kwargs):
    #     super().__init__()

    #     self.save_hyperparameters()

    #     model_params = self.hparams['model']
        
    #     self.model = object_from_dict(model_params)

        
    #     if "resume_from_checkpoint" in self.hparams:
    #         corrections = {"model.": ""}
            
    #         state_dict = state_diclt_from_disk(
    #             file_path=self.hparams["resume_from_checkpoint"],
    #             rename_in_layers=corrections,
    #         )
            
    #         self.model.load_state_dict(state_dict)
        
    #     self.losses = [
    #         ("crossEntropy", 0.5, SoftCrossEntropyLoss(smooth_factor=0.1)),
    #         ("diceloss", 2, DiceLoss(mode="multiclass", from_logits=True)),
    #     ]

    def __init__(self, hparams):
        super().__init__()

        self.hparams = hparams

        model_params = self.hparams['model']
        
        self.model = object_from_dict(model_params)

        
        if "resume_from_checkpoint" in self.hparams:
            corrections = {"model.": ""}
                
            state_dict = state_dict_from_disk(
                file_path=self.hparams["resume_from_checkpoint"],
                rename_in_layers=corrections,
            )
            
            self.model.load_state_dict(state_dict)

        self.train_losses = [
            ("facol", 0.5, FocalLoss(mode='multiclass')),
            ("diceloss", 0.5, DiceLoss(mode="multiclass", from_logits=True)),
        ]
        
        self.val_losses = [
            ("JaccardLoss", 0.5, JaccardLoss(mode="multiclass", from_logits=True)),
            ("diceloss", 0.5, DiceLoss(mode="multiclass", from_logits=True)),
        ]

    def forward(self, x):
        output = self.model(x)
        return output

    def train_dataloader(self):
        params = self.hparams["train_loader"]
        img_params = self.hparams["image"]

        dataset = SegDataset(params['data_size'], params['data_folder'], img_params['heigh'], img_params['width'], img_params['resize'], data_name='train', generate_new=params['generate_new'])

        result = DataLoader(dataset=dataset,
                            batch_size=self.hparams['batch_size'],
                            num_workers = params['num_workers'],
                            shuffle=True)
        
        return result

    def val_dataloader(self):
        params = self.hparams["val_loader"]
        img_params = self.hparams["image"]

        dataset = SegDataset(params['data_size'], params['data_folder'], img_params['heigh'], img_params['width'], img_params['resize'], data_name='val', generate_new=params['generate_new'])
        result = DataLoader(dataset=dataset,
                            batch_size=self.hparams['batch_size'],
                            shuffle=False)

        return result

    def training_step(self, batch, batch_idx):
        inputs, masks = batch
        logits = self(inputs)
        total_loss = 0
        for loss_name, weight, loss in self.train_losses:
            ls_mask = loss(logits, masks)
            total_loss += weight * ls_mask
            self.log(f"train_{loss_name}", ls_mask)

        self.log("train_loss", total_loss)
        self.log("lr", self._get_current_lr())
        
        return total_loss

    def _get_current_lr(self) -> torch.Tensor:
        lr = [x["lr"] for x in self.optimizers[0].param_groups][0]  # type: ignore
        return torch.Tensor([lr])[0].cuda()


    def validation_step(self, batch, batch_idx):
        inputs, masks = batch
        logits = self(inputs)
        total_loss = 0
        for loss_name, weight, loss in self.val_losses:
            ls_mask = loss(logits, masks)
            total_loss += weight * ls_mask
            self.log(f"val_{loss_name}", ls_mask)

        self.log("val_loss", total_loss)
        
        return total_loss
    
    def configure_optimizers(self):
        optimizer = object_from_dict(
            self.hparams["optimizer"],
            params=[x for x in self.model.parameters() if x.requires_grad],
        )

        scheduler = object_from_dict(self.hparams["scheduler"], optimizer=optimizer)
        self.optimizers = [optimizer]

        return self.optimizers, [scheduler]

def main(config, type='train'):

    with open(config) as f:
        hparams = yaml.load(f, Loader=yaml.SafeLoader)

    ##pipeline = SegTrain(**hparams)

    pipeline = SegTrain(hparams)

    checkpoint_callback=object_from_dict(hparams["checkpoint_callback"])
    early_stop_callback=object_from_dict(hparams["early_stop_callback"])

    trainer = pytorch_lightning.Trainer(callbacks=[checkpoint_callback, early_stop_callback], **hparams['trainer'])

    if type == 'train':
    
        trainer.fit(pipeline)

    else:

        trainer.test(pipeline)

        

    

if __name__ == '__main__':

    import argparse

    # help flag provides flag help
    # store_true actions stores argument as True

    parser = argparse.ArgumentParser()
   
    parser.add_argument('--config',dest='config',action='store', required=True,
                        help="config path")

    parser.add_argument('--type',dest='type',action='store',choices = ['train', 'test'], default='train', 
                        help="train or test")

    args = parser.parse_args()

    main(config=args.config, type=args.type)

    
